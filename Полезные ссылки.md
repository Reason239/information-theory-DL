#### Полезные ссылки

* [Статья Тишби](https://arxiv.org/abs/1703.00810) и [код к ней](https://github.com/ravidziv/IDNNs)
* [Новая статья, где оценивается MI через MINE и послойно тренируется сеть на основе этого](http://openaccess.thecvf.com/content_ICCVW_2019/html/SDL-CV/Elad_Direct_Validation_of_the_Information_Bottleneck_Principle_for_Deep_Nets_ICCVW_2019_paper.html)
  и [supplementary materials к ней](http://www.sdlcv-workshop.com/papers/19_supp.pdf)
* [Статья Saxe, где говорится, что выводы Тишби не подтверждаются для ReLU, ответ Тишби, что мол вы неправильно оцениваете информацию](https://openreview.net/forum?id=ry_WPG-A-)
  и [код к статье](https://github.com/artemyk/ibsgd)
* [Статья, где во вступлнии написано, что феномены у Тишби возникают только из-за биннинга и говорится, как померять всё по-другому](https://arxiv.org/abs/1810.05728v4)
* [Ещё способ измерить MI (верхняя грань)](https://arxiv.org/abs/1705.02436)
* [Блогпост вокруг темы IB](https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/)
* Статья, где говорится, что минимизирование функционала I - beta I переписывается по-другому, и позоже на оптимизацию кросс-энтропии плюс информация о входе; связь с автоэнкодерами https://arxiv.org/pdf/1904.03743.pdf и в ней ссылка на https://arxiv.org/pdf/1611.01353.pdfвуу
* Deep Infomax тоже использует версию MINE, но переработанную https://arxiv.org/abs/1808.06670 код: https://github.com/rdevon/DIM
* Очень старая статья https://pdfs.semanticscholar.org/fa95/df08439fb900427684f5d611429284596999.pdf
* How (Not) To Train Your Neural Network Using the Information Bottleneck Principle https://arxiv.org/abs/1802.09766v2
* Layer-wise Learning of Stochastic Neural Networks with Information Bottleneck https://arxiv.org/abs/1712.01272
* Parametric Information Bottleneck to Optimize Stochastic Neural Networks https://openreview.net/forum?id=ByED-X-0W
* Просто много статей около статьи Тишби http://www.arxiv-sanity.com/1703.00810v3
* Немного другие оценки MI, иногда показывают сжатие https://arxiv.org/abs/1902.09037
* Пост на Реддите https://www.reddit.com/r/MachineLearning/comments/be8qie/discussion_what_is_the_status_of_the_information/ там упоминается:
* Scalable Mutual Information Estimation using Dependence Graphs
 https://arxiv.org/abs/1801.09125
* Собственно, MINE (MI Neural Estimator) https://arxiv.org/abs/1801.04062
* Статья на тему IB после Тишби и Саксе https://arxiv.org/abs/1803.07980v2
