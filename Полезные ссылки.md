#### Полезные ссылки

* Статья Тишби: https://arxiv.org/pdf/1703.00810.pdf
  код: https://github.com/ravidziv/IDNNs
* Новая статья, где оценивается MI через MINE и тренируется сеть на основе этого: http://openaccess.thecvf.com/content_ICCVW_2019/html/SDL-CV/Elad_Direct_Validation_of_the_Information_Bottleneck_Principle_for_Deep_Nets_ICCVW_2019_paper.html
  Supplementary к ней: http://www.sdlcv-workshop.com/papers/19_supp.pdf
* Спор с Тишби и статья: https://openreview.net/forum?id=ry_WPG-A- 
  код: https://github.com/artemyk/ibsgd
* Ещё способ измерить MI (верхняя грань): https://arxiv.org/abs/1705.02436
* Блогпост вокруг всего https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/
* Статья, где говорится, что минимизирование функционала I - beta I переписывается по-другому, и позоже на оптимизацию кросс-энтропии плюс информация о входе; связь с автоэнкодерами https://arxiv.org/pdf/1904.03743.pdf и в ней ссылка на https://arxiv.org/pdf/1611.01353.pdfвуу
* Deep Infomax тоже использует версию MINE, но переработанную https://arxiv.org/abs/1808.06670
* Очень старая статья https://pdfs.semanticscholar.org/fa95/df08439fb900427684f5d611429284596999.pdf
* How (Not) To Train Your Neural Network Using the Information Bottleneck Principle https://arxiv.org/abs/1802.09766v2
* Layer-wise Learning of Stochastic Neural Networks with Information Bottleneck https://arxiv.org/abs/1712.01272
* Parametric Information Bottleneck to Optimize Stochastic Neural Networks https://openreview.net/forum?id=ByED-X-0W
* Просто много статей около статьи Тишби http://www.arxiv-sanity.com/1703.00810v3
* Здесь во вступлнии написано, что феномены Тишби возникают только из-за биннинга https://arxiv.org/abs/1810.05728v4
* Немного другие оценки MI, иногда показывают сжатие https://arxiv.org/abs/1902.09037
* Пост на Реддите https://www.reddit.com/r/MachineLearning/comments/be8qie/discussion_what_is_the_status_of_the_information/ там упоминается:
* Scalable Mutual Information Estimation using Dependence Graphs
 https://arxiv.org/abs/1801.09125
* Собственно, MINE (MI Neural Estimator) https://arxiv.org/abs/1801.04062
* Статья на тему IB после Тишби и Саксе https://arxiv.org/abs/1803.07980v2