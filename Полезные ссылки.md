#### Полезные ссылки

### Статьи, где сети тренируются, ориентируясь на информацию
* [Deep Infomax](https://arxiv.org/abs/1808.06670) тоже использует версию MINE для оценки информации в процессе обучения, но переработанный. [Код](https://github.com/rdevon/DIM)
* [Новая статья, где оценивается MI через MINE и послойно тренируется сеть на основе этого](http://openaccess.thecvf.com/content_ICCVW_2019/html/SDL-CV/Elad_Direct_Validation_of_the_Information_Bottleneck_Principle_for_Deep_Nets_ICCVW_2019_paper.html)
  и [supplementary materials к ней](http://www.sdlcv-workshop.com/papers/19_supp.pdf)
* [Ещё способ измерить MI (верхняя грань), тренировка сети на основе этого](https://arxiv.org/abs/1705.02436). Код: [TF](https://github.com/artemyk/nonlinearIB), [Pytorch](https://github.com/burklight/nonlinear-IB-PyTorch)
* [How (Not) To Train Your Neural Network Using the Information Bottleneck Principle](https://arxiv.org/abs/1802.09766v2)
* [Layer-wise Learning of Stochastic Neural Networks with Information Bottleneck](https://arxiv.org/abs/1712.01272)
* [Parametric Information Bottleneck to Optimize Stochastic Neural Networks (layer-wise)](https://openreview.net/forum?id=ByED-X-0W)

### Статьи на темы вокруг IB
* [Статья Тишби](https://arxiv.org/abs/1703.00810) и [код к ней](https://github.com/ravidziv/IDNNs)
* [Статья Saxe, где говорится, что выводы Тишби не подтверждаются для ReLU, ответ Тишби, что мол вы неправильно оцениваете информацию](https://openreview.net/forum?id=ry_WPG-A-)
  и [код к статье](https://github.com/artemyk/ibsgd)
* [Статья, где во вступлнии написано, что феномены у Тишби возникают только из-за биннинга и говорится, как померять всё по-другому](https://arxiv.org/abs/1810.05728v4)
* [Немного другие оценки MI, иногда показывают сжатие](https://arxiv.org/abs/1902.09037)
* Ещё подход к вычислению информации: [Scalable Mutual Information Estimation using Dependence Graphs](https://arxiv.org/abs/1801.09125)
* [Статья с обзором темы IB. В ней рассказано про проблемы с биннингом. Ещё говорится, что минимизация функционала $I(X, T) - \beta*I(T, Y)$ переписывается по-другому и похожа на оптимизацию кросс-энтропии плюс регуляризатор вроде информации; связь с автоэнкодерами](https://arxiv.org/abs/1904.03743) в ней ссылка [сюда](https://arxiv.org/abs/1611.01353), где есть о том же
* [Статья на тему IB после Тишби и Саксе](https://arxiv.org/abs/1803.07980v2)
* [Очень старая статья про информацию и DL](https://openreview.net/forum?id=rk-USD-u-r)

### MINE
* Собственно, MINE: [Mutual Information Neural Estimator](https://arxiv.org/abs/1801.04062)

### Куда ещё посмотреть
* [Блогпост вокруг темы IB](https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/)
* [Просто много статей около статьи Тишби на arXiv Sanity preserver](http://www.arxiv-sanity.com/1703.00810v3)
* [Пост на Реддите "What is the status of the "Information Bottleneck Theory of Deep Learning"](https://www.reddit.com/r/MachineLearning/comments/be8qie/discussion_what_is_the_status_of_the_information/)
