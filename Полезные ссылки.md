#### Полезные ссылки

* Статья Тишби: https://arxiv.org/pdf/1703.00810.pdf
  код: https://github.com/ravidziv/IDNNs
* Новая статья, где оценивается MI через MINE: http://openaccess.thecvf.com/content_ICCVW_2019/html/SDL-CV/Elad_Direct_Validation_of_the_Information_Bottleneck_Principle_for_Deep_Nets_ICCVW_2019_paper.html
  Supplementary к ней: http://www.sdlcv-workshop.com/papers/19_supp.pdf
* Спор с Тишби и статья: https://openreview.net/forum?id=ry_WPG-A- 
  код: https://github.com/artemyk/ibsgd
* Ещё способ измерить MI (верхняя грань): https://arxiv.org/abs/1705.02436
* Блогпост вокруг всего https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/
* Статья, где говорится, что минимизирование функционала I - beta I переписывается по-другому, и позоже на оптимизацию кросс-энтропии плюс информация о входе; связь с автоэнкодерами https://arxiv.org/pdf/1904.03743.pdf и в ней ссылка на https://arxiv.org/pdf/1611.01353.pdfвуу
* Deep Infomax тоже использует версию MINE, но переработанную https://arxiv.org/abs/1808.06670
* Очень старая статья https://pdfs.semanticscholar.org/fa95/df08439fb900427684f5d611429284596999.pdf
* How (Not) To Train Your Neural Network Using the Information Bottleneck Principle https://arxiv.org/abs/1802.09766v2
* Layer-wise Learning of Stochastic Neural Networks with Information Bottleneck https://arxiv.org/abs/1712.01272
* Parametric Information Bottleneck to Optimize Stochastic Neural Networks https://openreview.net/forum?id=ByED-X-0W